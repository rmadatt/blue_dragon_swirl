Absolutely! Let's create unit tests for your project. Unit tests focus on individual components of your code to ensure they work as expected in isolation. We'll place our unit tests in a directory called `tests/unit`. In this directory, we'll create separate test files for different modules or functionalities of your code.

Given the code you've provided earlier, we'll write unit tests for the following functions:

- `load_model` and `build_model` from your model utilities.
- `preprocess_image` from your image preprocessing utilities.
- `create_directory` and `load_config` from your configuration utilities.

---

### **Directory Structure**

First, let's visualize the directory structure:

```
your_project/
│
├── models/
│   └── your_model.h5
├── data/
│   └── images/
│       └── sample_image.jpg
├── src/
│   ├── model_utils.py          # Contains load_model and build_model
│   ├── image_utils.py          # Contains preprocess_image
│   └── config_utils.py         # Contains create_directory and load_config
└── tests/
    └── unit/
        ├── test_model_utils.py
        ├── test_image_utils.py
        └── test_config_utils.py
```

---

### **Unit Tests for `model_utils.py`**

Create a file named `test_model_utils.py` in `tests/unit/`:

```python
# tests/unit/test_model_utils.py

import unittest
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import os
import logging

# Import the functions to test
from src.model_utils import load_model, build_model

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class TestModelUtils(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.model_path = 'models/your_model.h5'
        cls.input_shape = (224, 224, 3)

        # Ensure the models directory exists
        if not os.path.exists('models'):
            os.makedirs('models')

        # Build and save a model for testing load_model
        model = build_model(cls.input_shape)
        model.save(cls.model_path)
        logging.info(f"Test model saved at {cls.model_path}")

    def test_build_model(self):
        """Test if build_model returns a valid Sequential model."""
        model = build_model(self.input_shape)
        self.assertIsInstance(model, Sequential, "build_model should return an instance of Sequential.")
        self.assertEqual(len(model.layers), 7, "Model should have 7 layers.")
        logging.info("test_build_model passed.")

    def test_load_model_success(self):
        """Test loading a model that exists."""
        model = load_model(self.model_path)
        self.assertIsNotNone(model, "Model should not be None when loading a valid model.")
        logging.info("test_load_model_success passed.")

    def test_load_model_failure(self):
        """Test handling when trying to load a non-existent model."""
        fake_model_path = 'models/non_existent_model.h5'
        model = load_model(fake_model_path)
        self.assertIsNone(model, "Model should be None when loading a non-existent model.")
        logging.info("test_load_model_failure passed.")

    @classmethod
    def tearDownClass(cls):
        # Clean up: remove the test model file
        if os.path.isfile(cls.model_path):
            os.remove(cls.model_path)
            logging.info(f"Removed test model file: {cls.model_path}")

if __name__ == '__main__':
    unittest.main()
```

---

### **Unit Tests for `image_utils.py`**

Create a file named `test_image_utils.py` in `tests/unit/`:

```python
# tests/unit/test_image_utils.py

import unittest
import numpy as np
import cv2
import os
import logging

# Import the function to test
from src.image_utils import preprocess_image

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class TestImageUtils(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.valid_image_path = 'data/images/sample_image.jpg'
        cls.invalid_image_path = 'data/images/invalid_image.jpg'
        cls.non_existent_image_path = 'data/images/non_existent.jpg'

        # Ensure the images directory exists
        if not os.path.exists('data/images'):
            os.makedirs('data/images')

        # Create a sample valid image
        if not os.path.isfile(cls.valid_image_path):
            sample_image = np.ones((500, 500, 3), dtype=np.uint8) * 255
            cv2.imwrite(cls.valid_image_path, sample_image)
            logging.info(f"Created sample valid image at {cls.valid_image_path}")

        # Create an invalid image file
        if not os.path.isfile(cls.invalid_image_path):
            with open(cls.invalid_image_path, 'w') as f:
                f.write("This is not a valid image file.")
            logging.info(f"Created invalid image file at {cls.invalid_image_path}")

    def test_preprocess_image_valid(self):
        """Test preprocessing a valid image."""
        processed_image = preprocess_image(self.valid_image_path)
        self.assertIsNotNone(processed_image, "Processed image should not be None for a valid image.")
        self.assertEqual(processed_image.shape, (224, 224, 3), "Processed image should have shape (224, 224, 3).")
        self.assertTrue(np.all((processed_image >= 0.0) & (processed_image <= 1.0)), "Processed image should be normalized between 0.0 and 1.0.")
        logging.info("test_preprocess_image_valid passed.")

    def test_preprocess_image_invalid(self):
        """Test preprocessing an invalid image file."""
        processed_image = preprocess_image(self.invalid_image_path)
        self.assertIsNone(processed_image, "Processed image should be None for an invalid image file.")
        logging.info("test_preprocess_image_invalid passed.")

    def test_preprocess_image_non_existent(self):
        """Test preprocessing a non-existent image file."""
        processed_image = preprocess_image(self.non_existent_image_path)
        self.assertIsNone(processed_image, "Processed image should be None for a non-existent image file.")
        logging.info("test_preprocess_image_non_existent passed.")

    @classmethod
    def tearDownClass(cls):
        # Clean up: remove the test images
        files_to_remove = [cls.valid_image_path, cls.invalid_image_path]
        for file_path in files_to_remove:
            if os.path.isfile(file_path):
                os.remove(file_path)
                logging.info(f"Removed test image file: {file_path}")

if __name__ == '__main__':
    unittest.main()
```

---

### **Unit Tests for `config_utils.py`**

Create a file named `test_config_utils.py` in `tests/unit/`:

```python
# tests/unit/test_config_utils.py

import unittest
import os
import json
import logging

# Import the functions to test
from src.config_utils import create_directory, load_config

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class TestConfigUtils(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.test_directory_path = 'data/test_directory'
        cls.valid_config_path = 'config/valid_config.json'
        cls.invalid_config_path = 'config/invalid_config.json'
        cls.non_existent_config_path = 'config/non_existent.json'

        # Ensure the config directory exists
        if not os.path.exists('config'):
            os.makedirs('config')

        # Create a valid JSON configuration file
        if not os.path.isfile(cls.valid_config_path):
            valid_config = {
                "param1": "value1",
                "param2": 42,
                "param3": [1, 2, 3]
            }
            with open(cls.valid_config_path, 'w') as f:
                json.dump(valid_config, f)
            logging.info(f"Created valid config file at {cls.valid_config_path}")

        # Create an invalid JSON configuration file
        if not os.path.isfile(cls.invalid_config_path):
            with open(cls.invalid_config_path, 'w') as f:
                f.write("{param1: value1, param2: 42, param3: [1, 2, 3]")  # Missing closing brace
            logging.info(f"Created invalid config file at {cls.invalid_config_path}")

    def test_create_directory_new(self):
        """Test creating a new directory."""
        if os.path.exists(self.test_directory_path):
            os.rmdir(self.test_directory_path)

        create_directory(self.test_directory_path)
        self.assertTrue(os.path.isdir(self.test_directory_path), "Directory should be created.")
        logging.info("test_create_directory_new passed.")

    def test_create_directory_existing(self):
        """Test creating a directory that already exists."""
        create_directory(self.test_directory_path)
        # Should not raise an exception
        self.assertTrue(os.path.isdir(self.test_directory_path), "Directory should exist.")
        logging.info("test_create_directory_existing passed.")

    def test_load_config_valid(self):
        """Test loading a valid configuration file."""
        config = load_config(self.valid_config_path)
        self.assertIsNotNone(config, "Config should not be None for a valid config file.")
        self.assertIsInstance(config, dict, "Config should be a dictionary.")
        self.assertEqual(config.get("param2"), 42, "Config parameter 'param2' should be 42.")
        logging.info("test_load_config_valid passed.")

    def test_load_config_invalid(self):
        """Test loading an invalid configuration file."""
        config = load_config(self.invalid_config_path)
        self.assertIsNone(config, "Config should be None for an invalid config file.")
        logging.info("test_load_config_invalid passed.")

    def test_load_config_non_existent(self):
        """Test loading a non-existent configuration file."""
        config = load_config(self.non_existent_config_path)
        self.assertIsNone(config, "Config should be None for a non-existent config file.")
        logging.info("test_load_config_non_existent passed.")

    @classmethod
    def tearDownClass(cls):
        # Clean up: remove the test directory and config files
        if os.path.isdir(cls.test_directory_path):
            os.rmdir(cls.test_directory_path)
            logging.info(f"Removed test directory: {cls.test_directory_path}")

        files_to_remove = [cls.valid_config_path, cls.invalid_config_path]
        for file_path in files_to_remove:
            if os.path.isfile(file_path):
                os.remove(file_path)
                logging.info(f"Removed config file: {file_path}")

if __name__ == '__main__':
    unittest.main()
```

---

### **Explanation of the Code**

**1. Imports and Configuration**

- **unittest**: Standard testing framework in Python.
- **Logging**: Provides feedback during test execution.
- **os**, **json**, **cv2**, **numpy**: Used for file operations and handling data.

**2. Importing Functions to Test**

- Each test file imports the specific functions it aims to test from your `src` module.

**3. `setUpClass` and `tearDownClass` Methods**

- **`setUpClass`**: Prepares the environment before tests run by creating necessary files and directories.
- **`tearDownClass`**: Cleans up after tests by deleting files and directories created during setup.

**4. Test Methods**

- Each test method is targeted at a specific functionality.
- **Naming Conventions**: Methods are named starting with `test_` followed by the function under test and the scenario.
- **Assertions**: Used to validate expected outcomes.

**5. Running the Tests**

- The `if __name__ == '__main__':` block allows the test file to be run directly.

---

### **How to Run Your Unit Tests**

1. **Navigate to the Project Directory**:

   ```bash
   cd your_project/
   ```

2. **Run Tests Individually**:

   ```bash
   python -m unittest tests/unit/test_model_utils.py
   python -m unittest tests/unit/test_image_utils.py
   python -m unittest tests/unit/test_config_utils.py
   ```

3. **Run All Unit Tests Together**:

   ```bash
   python -m unittest discover -s tests/unit
   ```

   This command discovers and runs all tests in the `tests/unit` directory.

---

### **Additional Recommendations**

**1. Use Virtual Environments**

Ensure you run your tests in a virtual environment to manage dependencies and avoid conflicts:

```bash
python -m venv venv
source venv/bin/activate  # On Windows, use venv\Scripts\activate
pip install -r requirements.txt
```

**2. Install Test Dependencies**

Make sure all necessary packages for testing are installed, such as `pytest` if you decide to use it later.

**3. Continuous Integration**

Integrate your tests into a CI/CD pipeline using tools like GitHub Actions, Travis CI, or Jenkins to automate testing on code changes.

**4. Code Coverage**

Use tools like `coverage.py` to measure how much of your code is exercised by your tests:

```bash
coverage run -m unittest discover -s tests/unit
coverage report -m
```

**5. Handle Randomness**

If your functions involve random elements (e.g., data augmentation), set random seeds or mock randomness to make your tests deterministic.

**6. Use Mocking for External Dependencies**

Consider using `unittest.mock` to simulate external dependencies or heavy computations in your unit tests.

---

### **Conclusion**

By creating these unit tests, you're ensuring that each component of your application works correctly in isolation. This makes debugging easier and increases the reliability of your codebase. As your project grows, continue to add unit tests for new functions and modules to maintain code quality.

---

### **What's Next?**

I'm excited to see your project taking shape! If you have any other code that needs unit tests or if you're interested in integration testing, performance optimization, or deployment strategies, let's explore those areas together.

Is there anything else you'd like assistance with?
